* Messy ideas
** DONE Idea 1: General code analyzer
CLOSED: [2025-11-05]

A command line tool that asks for the path to files to review, reads them, then asks a series of questions to clarify design decisions and understand the style of the programmer and not disrespect those intentions while still be helpful.

This should be configurable with an API key. You should also be able to seed your experience level as a programmer on first use and this should be stored in the configuration.

*** Implementation Status
- [X] Core CLI tool with Click framework
- [X] Configuration management (~/.config/code-tutor/config.json)
- [X] API key storage and management
- [X] Experience level configuration (beginner/intermediate/advanced/expert)
- [X] Question style preferences (socratic/direct/exploratory)
- [X] Focus areas selection (design, readability, performance, etc.)
- [X] File reading and language detection (30+ languages supported)
- [X] Interactive questioning system using Claude API
- [X] Personalized feedback based on user's answers
- [X] Follow-up conversation capability
- [X] Multi-file and directory review support
- [X] Rich formatted output using Rich library
- [X] Comprehensive documentation (DESIGN.md, README.md)

*** Project Structure
- src/code_tutor/ - Main package
  - cli.py - Command-line interface
  - config.py - Configuration management
  - file_reader.py - File reading and parsing
  - analyzer.py - Code analysis with Claude API
  - session.py - Interactive session management
- examples/ - Example files for testing
- DESIGN.md - Detailed design documentation
- pyproject.toml - Package configuration

*** Usage
#+begin_src bash
# Setup
code-tutor setup

# Review a file
code-tutor review examples/calculator.py

# Review a directory
code-tutor review src/

# View configuration
code-tutor config
#+end_src


** DONE Idea 2: Teach me!
CLOSED: [2025-11-05]

This should be an LLM mode that lets the user name a topic to talk about and then engages in a back and forth where it asks /the user/ to explain the topic by showing them (mistaken, misconcepted) code and then asking the user to identify what's wrong with it. This should be a separate tool than the reviewer, invoked by =code-tutor teach-me=.

The LLM should really work to be /cleverly/, instructively, wrong: make students have to /think/ about what's wrong and explain it.

If the LLM judges the explanation insufficiently precise, then "correct" the code so that it's still "cleverly" wrong and ask for further instruction the user until it's done.

*** Implementation Status
- [X] Teaching session module with Socratic method
- [X] CLI command: =code-tutor teach-me=
- [X] Topic and language selection
- [X] Intentional mistake generation with clever, instructive errors
- [X] User explanation collection
- [X] AI evaluation of explanation quality
- [X] Iterative refinement based on understanding
- [X] Multiple teaching rounds (up to 5)
- [X] Rich formatted output with syntax highlighting
- [X] Comprehensive documentation and examples

*** Key Features
- Socratic teaching approach - learn by identifying and explaining mistakes
- Dynamically generated flawed code specific to chosen topic
- Evaluation system that assesses depth of understanding
- Iterative learning - continues until concept is mastered
- Adaptive difficulty based on experience level
- Support for any programming language

*** Usage
#+begin_src bash
# Start teaching mode
code-tutor teach-me

# Example session:
# 1. Enter topic (e.g., "recursion", "async/await")
# 2. Choose language (e.g., "Python")
# 3. Review flawed code
# 4. Explain what's wrong
# 5. Get feedback
# 6. Continue with more examples or finish
#+end_src
